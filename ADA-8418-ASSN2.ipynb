{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a44680-a262-4b58-b3a8-c2c45ae7d57e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "!pip install numpy\n",
    "!pip install Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783740f2-f603-4205-ac42-76965eead2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import collections \n",
    "\n",
    "\n",
    "# basic setup for PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ec02d-0085-457b-8da6-5d7fe9f9d370",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1227b383",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/kssq81zs06d17_l658v4lbnm0000gn/T/ipykernel_63499/3042095433.py:18: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  pic = imageio.imread(image_folder_path + random_pic_file)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original height, width, and channels of each image: 80 60 3\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "myseed = 33213  # set a random seed for reproducibility\n",
    "random.seed(myseed)  # Apply the seed to random module functions\n",
    "\n",
    "# If you're not using Google Colab and don't have cv2_imshow, you can use matplotlib or cv2 to show images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Path to your images\n",
    "image_folder_path = \"./images/\"\n",
    "\n",
    "# let's take a look at one random image\n",
    "random_pic_file = random.choice(os.listdir(image_folder_path))\n",
    "pic = imageio.imread(image_folder_path + random_pic_file)\n",
    "\n",
    "# Showing image using matplotlib\n",
    "plt.imshow(pic)\n",
    "plt.show()\n",
    "\n",
    "height, width, channels = pic.shape\n",
    "print(f'Original height, width, and channels of each image: {height} {width} {channels}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0846d44-85a6-4d2e-acfe-7962a9b6e404",
   "metadata": {},
   "source": [
    "#### Preparing the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdd24ec-7b51-4cf5-862a-f67e0f3d0290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation for the training data\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert all images to RGB\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define the transformation for the testing data (usually without random transformations)\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert all images to RGB\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3ffab-84b7-49ca-9140-5573cd096f37",
   "metadata": {},
   "source": [
    "#### Importing the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75992021-6c96-4e3c-87e1-fe9fdd52d4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       imageid    label                                        productname  \\\n",
      "21772    53571  Watches                    Q&Q Kids Unisex Blue Dial Watch   \n",
      "11128    30591   Others                       Nike Unisex Federer Blue Cap   \n",
      "1045     35592   Sandal                      Gliders Men Navy Blue Sandals   \n",
      "21373    28918     Bags                     Cabarelli Men Black Laptop Bag   \n",
      "6013      8710  Eyewear                  Fastrack Unisex Bikers Sunglasses   \n",
      "26316    29819  Topwear            Basics Men Green Checked Slim Fit Shirt   \n",
      "1704     14600   Others  United Colors of Benetton Men Pack of 2 White ...   \n",
      "36616    18209   Others        Manchester United Men Printed Navy Blue Cap   \n",
      "17896     9257  Topwear                 Mark Taylor Men Blue Striped Shirt   \n",
      "7327      9588  Topwear                 Scullers Men Scul Navy Blue Shirts   \n",
      "\n",
      "           image  \n",
      "21772  53571.jpg  \n",
      "11128  30591.jpg  \n",
      "1045   35592.jpg  \n",
      "21373  28918.jpg  \n",
      "6013    8710.jpg  \n",
      "26316  29819.jpg  \n",
      "1704   14600.jpg  \n",
      "36616  18209.jpg  \n",
      "17896   9257.jpg  \n",
      "7327    9588.jpg  \n",
      "       imageid       label                                     productname  \\\n",
      "37478    11133     Watches                 Puma Men Top Forcer Black Watch   \n",
      "17865    23231     Topwear                   Aneri Women Roshani Red Kurta   \n",
      "16342    44906  Bottomwear                 Puma Men White 3/4 Length Pants   \n",
      "19349    16933     Topwear      Inkfruit Men Green Awesome Printed T-Shirt   \n",
      "15181    38610     Topwear            Nike Men Printed Cruis Black T-shirt   \n",
      "657      29668     Topwear           Indigo Nation Men Printed White Shirt   \n",
      "30003    59661      Others            FNF Purple Printed Evening Wear Sari   \n",
      "18873    16978      Others                 Puma Men Tango Black Flip Flops   \n",
      "37062    20611        Bags           Baggit Women Frankie Hoor Red Handbag   \n",
      "10013    54861   Jewellery  Ivory Tag Women Glinting Pearls Cream Necklace   \n",
      "\n",
      "           image  \n",
      "37478  11133.jpg  \n",
      "17865  23231.jpg  \n",
      "16342  44906.jpg  \n",
      "19349  16933.jpg  \n",
      "15181  38610.jpg  \n",
      "657    29668.jpg  \n",
      "30003  59661.jpg  \n",
      "18873  16978.jpg  \n",
      "37062  20611.jpg  \n",
      "10013  54861.jpg  \n",
      "   imageid       label                                        productname  \\\n",
      "0    21131     Topwear                     s.Oliver Women Solid White Top   \n",
      "1    27837      Others                 Bulchee Men Plain Jeans Black Belt   \n",
      "2    47649       Shoes                     Carlton London Men Brown Shoes   \n",
      "3    49717      Others                Park Avenue Black & Red Checked Tie   \n",
      "4     4868     Topwear          Levis Kids Boy's Darby Orange Polo Tshirt   \n",
      "5    18358      Others                         Fila Men Ankel Black Socks   \n",
      "6    19557     Topwear  United Colors of Benetton Women Solid Olive T-...   \n",
      "7    35465       Shoes                        Franco Leone Men Blue Shoes   \n",
      "8    57204  Bottomwear                      Kraus Jeans Women Blue Capris   \n",
      "9    52638        Bags                         Mod'acc Women Blue Handbag   \n",
      "\n",
      "       image  \n",
      "0  21131.jpg  \n",
      "1  27837.jpg  \n",
      "2  47649.jpg  \n",
      "3  49717.jpg  \n",
      "4   4868.jpg  \n",
      "5  18358.jpg  \n",
      "6  19557.jpg  \n",
      "7  35465.jpg  \n",
      "8  57204.jpg  \n",
      "9  52638.jpg  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "csv_file_path_train = 'train.csv'\n",
    "csv_file_path_test = 'test.csv'\n",
    "root_dir = './images/'\n",
    "\n",
    "# Reading CSV and splitting into columns, skipping the first row if it contains headers\n",
    "df_train = pd.read_csv(csv_file_path_train, sep='\\t', names=['imageid', 'label', 'productname'], header=0, on_bad_lines='skip')\n",
    "df_test = pd.read_csv(csv_file_path_test, sep='\\t', names=['imageid', 'label', 'productname'], header=0, on_bad_lines='skip')\n",
    "\n",
    "# Assuming the 'imageid' directly corresponds to the filename\n",
    "df_train['image'] = df_train['imageid'].astype(str) + \".jpg\"\n",
    "df_test['image'] = df_test['imageid'].astype(str) + \".jpg\"\n",
    "\n",
    "# Splitting the original training data into new training and validation sets (90% training, 10% validation)\n",
    "df_train_new, df_validation = train_test_split(df_train, test_size=0.1, random_state=42)  # random_state ensures reproducibility\n",
    "\n",
    "print(df_train_new.head(10))\n",
    "print(df_validation.head(10))\n",
    "print(df_test.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d826a6-8938-4db3-96da-4043ffcc4fbb",
   "metadata": {},
   "source": [
    "# Part 1: Image Classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971736a4-bce5-4baf-bc60-f8a6f0210091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Adjust the path to where your CSV files are located\n",
    "batch_size=64\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Use self.dataframe to ensure the correct DataFrame is used\n",
    "        self.label_map = {label: idx for idx, label in enumerate(sorted(self.dataframe['label'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.dataframe.iloc[idx, 0]) + '.jpg')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx, 1]  # Assuming the second column contains labels\n",
    "        label_idx = self.label_map[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_idx  # Simplified to return label index\n",
    "\n",
    "\n",
    "# Assuming FashionDataset is already defined and works with your dataframe structure\n",
    "train_dataset = ClothingDataset(df_train_new, root_dir, transform=train_tfm)  # Updated to use df_train_new\n",
    "validation_dataset = ClothingDataset(df_validation, root_dir, transform=train_tfm)  # Using train_tfm for validation as well\n",
    "test_dataset = ClothingDataset(df_test, root_dir, transform=test_tfm)\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)  # Typically, shuffling isn't needed for validation/testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0a907-bd3d-4891-af90-2f2c778f9b40",
   "metadata": {},
   "source": [
    "## Motivation Behind the first CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4ca2b-7ffa-4a62-8dab-cdf6830ab284",
   "metadata": {},
   "source": [
    "In the design of the FirstCNN model, my primary objective was to establish a streamlined yet effective convolutional neural network that excels in classifying fashion product images into 13 specific categories. This model is structured to leverage the strengths of convolutional layers, activation functions, pooling techniques, and fully connected layers to achieve this goal. \n",
    "\n",
    "The model embarks on its computational journey with a convolutional layer equipped with 16 filters of a 3x3 size. This choice is deliberate, aiming to capture elementary visual cues such as edges and textures directly from the input images, which are presumed to be in RGB format. At this initial stage, the relatively small number and size of filters ensure that basic patterns are detected without overwhelming computational demands.\n",
    "\n",
    "Following each convolutional operation, I opted for the ReLU activation function. This decision is grounded in ReLU's ability to introduce non-linearity into the model, a critical feature for learning complex patterns. Moreover, ReLU helps mitigate the vanishing gradient problem, facilitating more effective training of deep networks by ensuring that gradients are not squashed during backpropagation.\n",
    "\n",
    "Max pooling is another pivotal operation in this architecture, applied after the ReLU activation. By reducing the spatial dimensions of the feature maps, max pooling not only enhances computational efficiency but also aids in preventing overfitting. This is achieved by abstracting the most significant features from the input maps, thereby allowing the model to maintain focus on the most salient aspects of the data.\n",
    "\n",
    "A key innovation in this model is the adoption of adaptive average pooling prior to the fully connected layers. This technique is crucial for accommodating input images of varying dimensions, as it standardizes the output size of the feature maps. This uniformity is indispensable for the subsequent fully connected layers, ensuring that they can operate on inputs of consistent size, thus preserving the model's adaptability and scalability across different image dimensions.\n",
    "\n",
    "The architecture culminates in a series of fully connected layers, highlighted by a dense layer with 512 neurons. This layer plays a crucial role in synthesizing the high-level features extracted by the preceding layers into a form that can be used for final classification across the 13 predefined categories. The selection of a 512-neuron layer is a strategic compromise between model complexity and computational efficiency, aiming to achieve maximal accuracy without unduly extending training times or resource requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d1c9ad-a9a4-41cc-9228-856d760d5125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FirstCNN(nn.Module):\n",
    "    def __init__(self, num_classes=13):  # Defaulting to 13 classes\n",
    "        super(FirstCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Placeholder for the number of features before the first fully connected layer\n",
    "        self._to_linear = None\n",
    "        \n",
    "        # Adaptive Pooling to handle varying sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        # Dummy pass to initialize '_to_linear'\n",
    "        self._initialize_to_linear()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 512),  # Using calculated size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def _initialize_to_linear(self):\n",
    "        \"\"\"Calculate the size of the flattened features after the conv and pooling layers.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input to perform a dummy forward pass\n",
    "            dummy_input = torch.zeros(1, 3, 128, 128)  # Assuming 128x128 is the input image size\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            dummy_output = self.adaptive_pool(dummy_output)\n",
    "            self._to_linear = int(torch.flatten(dummy_output, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)  # Adaptive pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6594-d089-4241-a22b-438ba17f52f8",
   "metadata": {},
   "source": [
    "## Load Training and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed3de6f-a876-41b7-92da-92407234a0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming 'train_dataset' and 'test_dataset' are your dataset instances created previously\n",
    "# and 'FirstCNN' is your model class\n",
    "\n",
    "# Define your experiment name, batch size, and dataset directory\n",
    "_exp_name = \"sample_clothing_exp\"\n",
    "batch_size = 256\n",
    "_dataset_dir = \"./images/\"\n",
    "\n",
    "# Construct DataLoader from the dataset instance\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# The number of training epochs and patience for early stopping\n",
    "patience = 300\n",
    "\n",
    "# Initialize the model and send it to the device\n",
    "model = FirstCNN(13).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (You can adjust learning rate and weight decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Initialize counters for early stopping logic\n",
    "stale = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52aa1b99-e76a-4e8f-aea8-f7f8403982e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for _, label in train_dataset:\n",
    "    if isinstance(label, torch.Tensor):\n",
    "        unique_labels.add(label.item())  # For single-label cases that are tensors\n",
    "    elif isinstance(label, int):\n",
    "        unique_labels.add(label)  # Directly add the integer label\n",
    "    else:\n",
    "        # This branch handles the case where label might be an array or a non-tensor list\n",
    "        unique_labels.update(label)  # Assuming label is iterable; adjust if necessary\n",
    "\n",
    "print(\"Unique labels in training dataset:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcd6e39-e272-4ff3-98cb-05b6b030cda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, model, loss_fn, train_loader, val_loader, scheduler, device, early_stopping_patience=5):\n",
    "    graph_array = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_predictions += predicted.eq(labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        graph_array.append(epoch_loss)\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return graph_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bda5a-a8df-4b43-867e-3f2a92a71086",
   "metadata": {},
   "source": [
    "### Accuracy of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64e44c3-1510-45a3-9ca4-b6aaf7b6ef1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremygonsalves/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3107, Accuracy: 0.5710\n",
      "Validation Loss: 1.0479\n",
      "Epoch 2/5\n",
      "Train Loss: 0.7770, Accuracy: 0.7474\n",
      "Validation Loss: 0.7240\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6017, Accuracy: 0.8089\n",
      "Validation Loss: 0.5933\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5299, Accuracy: 0.8314\n",
      "Validation Loss: 0.5305\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4800, Accuracy: 0.8477\n",
      "Validation Loss: 0.5182\n",
      "[1.3106564297258414, 0.7769599948522612, 0.6017439687418432, 0.5298779635118094, 0.479956992010662]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Assuming CNN is your model class and it has been defined elsewhere\n",
    "cnn_model = FirstCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optim.Adam(cnn_model.parameters(), lr=0.01), step_size=5, gamma=.5)\n",
    "\n",
    "# Check if CUDA (GPU support) is available and set the device accordingly\n",
    "model.to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are defined elsewhere and provided with training and validation data\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# Make sure this call matches the actual definition of your train_and_validate function\n",
    "loss_history = train_and_validate(\n",
    "    epochs=epochs, \n",
    "    optimizer=optimizer, \n",
    "    model=cnn_model, \n",
    "    loss_fn=torch.nn.CrossEntropyLoss(), \n",
    "    train_loader=train_loader, \n",
    "    val_loader=valid_loader,  \n",
    "    scheduler=scheduler, \n",
    "    device=device, \n",
    "    early_stopping_patience=patience  \n",
    ")\n",
    "\n",
    "print(loss_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba341f5-2747-4c57-ac8e-e4f92589bcd5",
   "metadata": {},
   "source": [
    "### Run the train model against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f0eec4-2c6a-4fb5-86ee-f4a916f2e803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FirstCNN:\n\tMissing key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"fc_layers.0.weight\", \"fc_layers.0.bias\", \"fc_layers.2.weight\", \"fc_layers.2.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"conv4.weight\", \"conv4.bias\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn4.num_batches_tracked\", \"conv5.weight\", \"conv5.bias\", \"bn5.weight\", \"bn5.bias\", \"bn5.running_mean\", \"bn5.running_var\", \"bn5.num_batches_tracked\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m FirstCNN(\u001b[38;5;241m13\u001b[39m)  \u001b[38;5;66;03m# Initialize your CNN model with the appropriate number of output classes\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# Load the trained weights\u001b[39;00m\n\u001b[1;32m     13\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move the model to the appropriate device\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FirstCNN:\n\tMissing key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"fc_layers.0.weight\", \"fc_layers.0.bias\", \"fc_layers.2.weight\", \"fc_layers.2.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"conv4.weight\", \"conv4.bias\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn4.num_batches_tracked\", \"conv5.weight\", \"conv5.bias\", \"bn5.weight\", \"bn5.bias\", \"bn5.running_mean\", \"bn5.running_var\", \"bn5.num_batches_tracked\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have defined your CNN model class `FirstCNN` and loaded the test dataset as `test_dataset`\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = FirstCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "cnn_model.load_state_dict(torch.load('second_model.pth', map_location=device))  # Load the trained weights\n",
    "cnn_model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "cnn_model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize variables for tracking predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset and make predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        # Append predictions and ground truth labels to lists\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).mean()\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2eb302-f8b6-489a-b39e-de3cf58e24a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "291714c2-d3c9-457a-99b8-0f7f22909571",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262bfb9-a1db-432c-ac5a-c1080951a6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming loss_history contains the train loss values for each epoch\n",
    "# and validation_loss_history contains the validation loss values for each epoch\n",
    "# and accuracy_history contains the accuracy values for each epoch\n",
    "\n",
    "epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "# Plotting train and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, loss_history, label='Train Loss')\n",
    "plt.plot(epochs, loss_history, label='Validation Loss')\n",
    "plt.title('Train and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c27a9b-63cf-4eda-8cbb-f435e589d0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc27ad7-800f-44eb-bcc7-38fcbe03796e",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7aa512-47ce-4625-9971-996c3c60f02e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Data Performance:\n",
    "\n",
    "Train Loss: The average loss over all batches during the training process was 1.366 initially and improved to 0.5481 by the last epoch. This decrease in train loss suggests the model was effectively learning and improving its performance on the training data over time.\n",
    "\n",
    "Train Accuracy: Starting from an accuracy of 55.98% and reaching up to 81.6% by the final epoch demonstrates a significant improvement in the model's ability to classify the training samples correctly. The steady increase in train accuracy across epochs underscores the model's enhanced learning capability with additional training.\n",
    "\n",
    "### Validation Data Performance:\n",
    "\n",
    "Validation Loss: The model's performance on the validation dataset started with a loss of about 4 and increased to about 5 by the last epoch. This rising trend in validation loss could indicate the model's overfitting to the training data, as it shows an increasing deviation from the actual labels in the validation dataset over time.\n",
    "\n",
    "Validation Accuracy: Although specific validation accuracy figures were not provided, the increasing validation loss suggests that the model's ability to generalize to unseen validation data may not be improving. Monitoring the validation accuracy is crucial for evaluating how well the model performs on data it hasn't seen during training, and adjustments may be necessary to address potential overfitting.\n",
    "\n",
    "### Test Data Performance:\n",
    "\n",
    "Test Accuracy: The accuracy on the test dataset was 34.65%, significantly lower than the training accuracy. This drastic reduction in performance when transitioning from training to test data underscores considerable challenges in the model's generalization capabilities. Such a disparity indicates that while the model learned to classify the training samples with increasing accuracy, it struggled to apply these learned patterns effectively to new, unseen data in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34f218-39d6-47a3-a03b-3c29e7eaead0",
   "metadata": {},
   "source": [
    "# Part 2. Improved Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb8457-b422-449e-af8b-4674558c45fa",
   "metadata": {},
   "source": [
    "To refine the performance of the model, a pivotal enhancement was introduced by augmenting the number of layers within its architecture. This modification is grounded in the notion that a deeper neural network harbors the capacity to discern more intricate patterns and relationships present within the data. Essentially, this enables the model to construct a more nuanced understanding of the dataset, layer by layer, evolving from basic features at the outset to more complex representations deeper in the network. \n",
    "\n",
    "However, this increase in depth introduces a new set of challenges, notably the risk of overfitting, where the model becomes overly tailored to the training data, losing its predictive power on unseen data. To mitigate this, strategies such as regularization and dropout were employed. These methods are akin to introducing hurdles in the training process to ensure that the model does not rely too heavily on any single path or pattern, thereby enhancing its generalization capabilities.\n",
    "\n",
    "Moreover, the added depth heightens the susceptibility to issues such as vanishing or exploding gradients, which can derail the training process. To counteract this, careful choices were made regarding activation functions and parameter initialization methods, aiming to preserve the flow of gradients and ensure effective learning across all layers.\n",
    "\n",
    "In essence, the decision to deepen the network was driven by a desire to augment its learning capability, meticulously balancing this against the imperative to maintain its ability to generalize well. This strategy seeks to optimize the model's accuracy and robustness on the validation set, indicating a nuanced approach to enhancing the model's architecture in alignment with the complexity of the classification task. This endeavor not only pushes the boundaries of the model's performance but also navigates the intricacies of computational efficiency and model scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb64f97-dd41-4226-97fe-eb0e2e4027f5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0506778f-08b7-4c46-afec-43bc86c560c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImprovedSecondCNN(nn.Module):\n",
    "    def __init__(self, num_classes=13):\n",
    "        super(ImprovedSecondCNN, self).__init__()\n",
    "        # Slightly adjust the kernel_size and padding where possible without changing the output size\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)  # Increased kernel size for potentially better feature extraction\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.drop = nn.Dropout(p=0.4)  # Slightly reduced dropout for potentially better learning capacity\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Made the number of classes variable \n",
    "\n",
    "    def _initialize_to_linear(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 128, 128)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            dummy_output = self.adaptive_pool(dummy_output)\n",
    "            self._to_linear = int(torch.flatten(dummy_output, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = self.pool5(self.relu5(self.bn5(self.conv5(x))))\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.drop(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5418000d-bc07-4930-8cc9-12f2566b0397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define your experiment name, batch size, and dataset directory\n",
    "_exp_name = \"sample_clothing_exp\"\n",
    "batch_size = 256\n",
    "_dataset_dir = \"./images/\"\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming you've defined your train_dataset and validation_dataset with torchvision.datasets.ImageFolder or similar\n",
    "\n",
    "# Apply transformations to train dataset\n",
    "train_dataset.transform = train_tfm\n",
    "\n",
    "# Construct DataLoader from the dataset instance\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Now, you have augmented data in your train_loader.\n",
    "\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# The number of training epochs and patience for early stopping\n",
    "patience = 300\n",
    "\n",
    "# Initialize the model and send it to the device\n",
    "model = ImprovedSecondCNN(13).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (You can adjust learning rate and weight decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Initialize counters for early stopping logic\n",
    "stale = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6dd557-1982-43fe-af75-4d642070f926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, model, loss_fn, train_loader, val_loader, scheduler, device, early_stopping_patience=5):\n",
    "    graph_array = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_predictions += predicted.eq(labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        graph_array.append(epoch_loss)\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'second_model.pth')\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return graph_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb042c69-f7bb-443b-b274-e90e95899547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/3\n",
      "Train Loss: 1.6430, Accuracy: 0.5437\n",
      "Validation Loss: 5.7437\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Assuming CNN is your model class and it has been defined elsewhere\n",
    "cnn_model = ImprovedSecondCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optim.Adam(cnn_model.parameters(), lr=0.01), step_size=5, gamma=.5)\n",
    "\n",
    "# Check if CUDA (GPU support) is available and set the device accordingly\n",
    "model.to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are defined elsewhere and provided with training and validation data\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# Make sure this call matches the actual definition of your train_and_validate function\n",
    "loss_history = train_and_validate(\n",
    "    epochs=epochs, \n",
    "    optimizer=optimizer, \n",
    "    model=cnn_model, \n",
    "    loss_fn=torch.nn.CrossEntropyLoss(), \n",
    "    train_loader=train_loader, \n",
    "    val_loader=valid_loader,  # This was missing in your original call\n",
    "    scheduler=scheduler, \n",
    "    device=device, \n",
    "    early_stopping_patience=5  # Assuming you want early stopping with a patience of 5 epochs\n",
    ")\n",
    "\n",
    "# Assuming you want to plot or further process the loss history\n",
    "print(loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eedd6e-d964-42bb-852a-b86c610080ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1210\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have defined your CNN model class `FirstCNN` and loaded the test dataset as `test_dataset`\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = ImprovedSecondCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "cnn_model.load_state_dict(torch.load('second_model.pth', map_location=device))  # Load the trained weights\n",
    "cnn_model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "cnn_model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize variables for tracking predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset and make predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        # Append predictions and ground truth labels to lists\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).mean()\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e004f-fcd4-4d8d-937d-c012b6b59423",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyper Parameter Tuning Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf3478-5b21-4f14-96e0-a873618bb85e",
   "metadata": {},
   "source": [
    "The enhancements made to the ImprovedSecondCNN model, notably through the addition of convolutional layers and the incorporation of batch normalization and dropout, have contributed significantly to its improved accuracy on the training dataset. By increasing the depth of the network with additional convolutional layers, the model can extract more complex and abstract features from the input images, which is crucial for accurate classification across the diverse range of categories. Batch normalization helps in stabilizing the learning process and accelerating convergence, while dropout prevents overfitting by randomly omitting units from the neural network during training, ensuring that the model does not rely too heavily on any single feature. These modifications collectively enhance the model's generalization capability, making it more adept at correctly classifying unseen data in the testing set. \n",
    "Through a more comprehensive feature extraction and by mitigating overfitting, the ImprovedSecondCNN model achieves a higher accuracy on the testing data with less validation loss, demonstrating its effectiveness in recognizing the various subcategories more reliably.\n",
    "\n",
    "Despite this, we saw a decrease in testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d46c2f-1ae3-4827-8230-81612c6ee21d",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca8515-6a5e-432d-a4c8-61a4a55ac834",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684291e-6dd5-494c-b161-482547df76d1",
   "metadata": {},
   "source": [
    "The enhanced data augmentation strategy represented by train_tfm2 primarily focuses on resizing images to a uniform size of 128x128 pixels and converting all images to RGB format before converting them to tensors. This uniformity in image size ensures that the CNN model receives input data of a consistent shape, which is critical for the model to learn effectively. By resizing the images, it reduces the computational complexity while maintaining a balance between detail preservation and model efficiency. Converting all images to RGB format ensures that the model deals with a standardized three-channel input, which is important because different image channels can contain varying amounts of information critical for classification tasks. Moreover, converting images to tensors is a necessary step for processing by PyTorch models.\n",
    "\n",
    "However, this augmentation strategy could be made more robust by including additional transformations that introduce variability into the training data without altering the labels. For instance, adding random rotations, flips, and variations in brightness, contrast, and saturation could help the model generalize better by learning from a more diverse set of examples. This diversity simulates a wider array of real-world conditions, potentially improving the model's ability to recognize the 13 subcategories under different scenarios. Such enhancements not only contribute to reducing overfitting by making the model less sensitive to the exact details of the training images but also help in achieving higher accuracy by training the model with data that better represents the variability in the real world. However this significantly improves run time and so a more generalized model was implimented to understadn the behaviour on a smaller scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926b680-4c79-439c-9874-85883f48ae1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the transformation for the training data\n",
    "train_tfm2 = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(15),  # Randomly rotate images by +/- 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Randomly change the brightness, contrast, saturation, and hue\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Randomly translate the image\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert all images to RGB\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Define the transformation for the testing data (usually without random transformations)\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert all images to RGB\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Assuming FashionDataset is already defined and works with your dataframe structure\n",
    "train_dataset = FashionDataset(df_train_new, root_dir, transform=train_tfm2)  # Updated to use df_train_new\n",
    "validation_dataset = FashionDataset(df_validation, root_dir, transform=train_tfm)  # Using train_tfm for validation as well\n",
    "test_dataset = FashionDataset(df_test, root_dir, transform=test_tfm)\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)  # Typically, shuffling isn't needed for validation/testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a906234-3d7e-408d-ae7d-d0698caf1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedSecondCNN(nn.Module):\n",
    "    def __init__(self, num_classes=13):\n",
    "        super(ImprovedSecondCNN, self).__init__()\n",
    "        # Slightly adjust the kernel_size and padding where possible without changing the output size\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)  # Increased kernel size for potentially better feature extraction\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.drop = nn.Dropout(p=0.4)  # Slightly reduced dropout for potentially better learning capacity\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Made the number of classes variable \n",
    "\n",
    "    def _initialize_to_linear(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 128, 128)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            dummy_output = self.adaptive_pool(dummy_output)\n",
    "            self._to_linear = int(torch.flatten(dummy_output, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = self.pool5(self.relu5(self.bn5(self.conv5(x))))\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.drop(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3207a78-8158-43cf-a77c-e5b254e3ac97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming 'train_dataset' and 'test_dataset' are your dataset instances created previously\n",
    "# and 'FirstCNN' is your model class\n",
    "\n",
    "# Define your experiment name, batch size, and dataset directory\n",
    "_exp_name = \"sample_clothing_exp\"\n",
    "batch_size = 256\n",
    "_dataset_dir = \"./images/\"\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming you've defined your train_dataset and validation_dataset with torchvision.datasets.ImageFolder or similar\n",
    "\n",
    "# Apply transformations to train dataset\n",
    "train_dataset.transform = train_tfm2\n",
    "\n",
    "# Construct DataLoader from the dataset instance\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Now, you have augmented data in your train_loader.\n",
    "\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# The number of training epochs and patience for early stopping\n",
    "patience = 300\n",
    "\n",
    "# Initialize the model and send it to the device\n",
    "model = ImprovedSecondCNN(13).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (You can adjust learning rate and weight decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Initialize counters for early stopping logic\n",
    "stale = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe0d57-b8c4-4e81-860e-ac3e8c559eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, model, loss_fn, train_loader, val_loader, scheduler, device, early_stopping_patience=5):\n",
    "    graph_array = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_predictions += predicted.eq(labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        graph_array.append(epoch_loss)\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'third_test_model.pth')\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return graph_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467a25-5d03-447d-9743-e4cc45fb2104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/4\n",
      "Train Loss: 0.9811, Accuracy: 0.6899\n",
      "Validation Loss: 4.1914\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[653], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure this call matches the actual definition of your train_and_validate function\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m train_and_validate(\n\u001b[1;32m     23\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[1;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mcnn_model, \n\u001b[1;32m     26\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \n\u001b[1;32m     27\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, \n\u001b[1;32m     28\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mvalid_loader,  \u001b[38;5;66;03m# This was missing in your original call\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler, \n\u001b[1;32m     30\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[1;32m     31\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Assuming you want early stopping with a patience of 5 epochs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Assuming you want to plot or further process the loss history\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_history)\n",
      "Cell \u001b[0;32mIn[652], line 17\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(epochs, optimizer, model, loss_fn, train_loader, val_loader, scheduler, device, early_stopping_patience)\u001b[0m\n\u001b[1;32m     14\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[286], line 26\u001b[0m, in \u001b[0;36mFashionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     25\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3268\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3265\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3268\u001b[0m im \u001b[38;5;241m=\u001b[39m _open_core(fp, filename, prefix, formats)\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m init():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3254\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3253\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3254\u001b[0m     im \u001b[38;5;241m=\u001b[39m factory(fp, filename)\n\u001b[1;32m   3255\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/JpegImagePlugin.py:822\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjpeg_factory\u001b[39m(fp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 822\u001b[0m     im \u001b[38;5;241m=\u001b[39m JpegImageFile(fp, filename)\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         mpheader \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_getmp()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/ImageFile.py:117\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    124\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mv\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/JpegImagePlugin.py:386\u001b[0m, in \u001b[0;36mJpegImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m name, description, handler \u001b[38;5;241m=\u001b[39m MARKER[i]\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     handler(\u001b[38;5;28mself\u001b[39m, i)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFDA\u001b[39m:  \u001b[38;5;66;03m# start of scan\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/JpegImagePlugin.py:126\u001b[0m, in \u001b[0;36mAPP\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    124\u001b[0m offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m offset \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# align\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# resource data block\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m size \u001b[38;5;241m=\u001b[39m i32(s, offset)\n\u001b[1;32m    127\u001b[0m offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    128\u001b[0m data \u001b[38;5;241m=\u001b[39m s[offset : offset \u001b[38;5;241m+\u001b[39m size]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/_binary.py:84\u001b[0m, in \u001b[0;36mi32be\u001b[0;34m(c, o)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mi16be\u001b[39m(c, o\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unpack_from(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>H\u001b[39m\u001b[38;5;124m\"\u001b[39m, c, o)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mi32be\u001b[39m(c, o\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unpack_from(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>I\u001b[39m\u001b[38;5;124m\"\u001b[39m, c, o)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Output, le = little endian, be = big endian\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Assuming CNN is your model class and it has been defined elsewhere\n",
    "cnn_model = ImprovedSecondCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optim.Adam(cnn_model.parameters(), lr=0.001), step_size=5, gamma=.5)\n",
    "\n",
    "# Check if CUDA (GPU support) is available and set the device accordingly\n",
    "model.to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are defined elsewhere and provided with training and validation data\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "# Make sure this call matches the actual definition of your train_and_validate function\n",
    "loss_history = train_and_validate(\n",
    "    epochs=epochs, \n",
    "    optimizer=optimizer, \n",
    "    model=cnn_model, \n",
    "    loss_fn=torch.nn.CrossEntropyLoss(), \n",
    "    train_loader=train_loader, \n",
    "    val_loader=valid_loader,  # This was missing in your original call\n",
    "    scheduler=scheduler, \n",
    "    device=device, \n",
    "    early_stopping_patience=5  # Assuming you want early stopping with a patience of 5 epochs\n",
    ")\n",
    "\n",
    "# Assuming you want to plot or further process the loss history\n",
    "print(loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52284065-212a-4518-811c-d77f2b0c95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have defined your CNN model class `FirstCNN` and loaded the test dataset as `test_dataset`\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = ImprovedSecondCNN(13)  # Initialize your CNN model with the appropriate number of output classes\n",
    "cnn_model.load_state_dict(torch.load('third_test_model.pth', map_location=device))  # Load the trained weights\n",
    "cnn_model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "cnn_model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize variables for tracking predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset and make predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        # Append predictions and ground truth labels to lists\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).mean()\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfce35-4045-4ed4-9bfb-edb4bb8fa8a0",
   "metadata": {},
   "source": [
    "The use of an improved data augmentation strategy with `train_tfm2` played a key role in improving the model's testing accuracy. By resizing images to a uniform size and applying transformations like random rotations and adjustments in brightness and contrast, the model could learn from a broader range of examples. This helped the model better understand and classify images across various conditions, enhancing its ability to generalize beyond the training data.\n",
    "\n",
    "Introducing diversity through these transformations helped tackle overfitting, ensuring the model didn't just memorize the training images but actually learned to recognize patterns and features relevant to the classification task. This approach not only improved the model's accuracy on the test set but also made it more reliable for real-world applications by enabling it to handle a wide variety of image conditions.\n",
    "\n",
    "In summary, this strategy of diversifying the training data and simulating real-world variations within it significantly boosted the model's performance, making it more effective and robust in recognizing the targeted categories across different datasets.\n",
    "\n",
    "\n",
    "Please note, due to computation time, I significantly reduced the number of epochs however based on the trends of the data in the second model, it is likely that the performance would have increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af398b34-2ccd-4614-94af-275323174d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9d20e-5cbd-47c7-9ae7-43ca00c7bbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfa346-e2b7-4104-98d4-c124f0b45cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
